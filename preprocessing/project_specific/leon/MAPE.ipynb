{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Average Percentange Error (MAPE) calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to interpolate between six stationary sensor values, perform k-fold cross validation on CNN predictions, and map statistical and CNN prediction results. Mean Average Percentage Error (MAPE) is calculated for interpolation and CNN predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from constants import Constants\n",
    "from grid_definition import GridDefinition\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from osm_reader import OSMReader\n",
    "import folium\n",
    "from data_download import DataDownloader\n",
    "import random\n",
    "\n",
    "import pykrige.kriging_tools as kt\n",
    "from pykrige.uk import UniversalKriging\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from scipy.interpolate import Rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = Constants()\n",
    "gridDefinition = GridDefinition()\n",
    "gridDefinition.init()\n",
    "osm_reader = OSMReader()\n",
    "osm_reader.init()\n",
    "\n",
    "corners = constants.getCorners()\n",
    "staticCoords = constants.getStaticCoords()\n",
    "GRID_SIZE = constants.getGridSize()\n",
    "\n",
    "topLat = gridDefinition.getTopLat()\n",
    "leftLon = gridDefinition.getLeftLon()\n",
    "\n",
    "bottomLat = gridDefinition.getBottomLat()\n",
    "rightLon = gridDefinition.getRightLon()\n",
    "\n",
    "height = topLat - bottomLat\n",
    "width = rightLon - leftLon\n",
    "\n",
    "heightInterval = height / GRID_SIZE\n",
    "widthInterval = width / GRID_SIZE\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "data_downloader = DataDownloader()\n",
    "\n",
    "PMstrs = ['PM1', 'PM2.5', 'PM10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate MAPE for Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "MAPPING_ON = False\n",
    "DOUBLE_COLLECTION = False\n",
    "interp_strs = [\"linear\", \"smooth\", \"UK\", \"OK\", \"RBF\"]\n",
    "\n",
    "data_collected = constants.getNineteenDates() \n",
    "\n",
    "if (MAPPING_ON):\n",
    "    cells = osm_reader.getGeoCells()\n",
    "\n",
    "for toDelete in range(1):\n",
    "    pm = 1\n",
    "    for i in [0, 1, 2, 3, 4]:\n",
    "        INTERP_TYPE = i\n",
    "        errors = []\n",
    "        for start_date in data_collected:\n",
    "            interpolate_PM(start_date, pm, 0)\n",
    "        \n",
    "        if (errors):\n",
    "            print(str(interp_strs[i]) + \",\" +  str(np.average(errors)))\n",
    "            \n",
    "        print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate MAPE for CNN Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_collected = constants.getNineteenDates() \n",
    "\n",
    "kernel_sizes = [1, 2, 3]\n",
    "learning_rate = 0.005 \n",
    "\n",
    "MAPPING_ON = False\n",
    "\n",
    "if (MAPPING_ON):\n",
    "    cells = osm_reader.getGeoCells()\n",
    "    \n",
    "i = 1\n",
    "print(PMstrs[i] + \", kernel size: \" + str(kernel_size) +\", learning_rate: \" + str(learning_rate))\n",
    "\n",
    "for kernel_size in kernel_sizes:\n",
    "    errors = []\n",
    "    r2s = []\n",
    "    \n",
    "    for date in data_collected:\n",
    "        ML_dir = '../results/19tune/learning_rate/learning_rate_' +str(learning_rate) + '/kernel_size_' +str(kernel_size) + '/valid_date_'+str(date)+'/'\n",
    "        ML_pred_file = ML_dir + \"PM2.5_prediction.csv\"\n",
    "\n",
    "        pd_df=pd.read_csv(ML_pred_file, sep=' ',header=None)\n",
    "        PM_pred = pd_df.values\n",
    "        label_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/data/label/\"+str(date)+\"/\"\n",
    "        sids = ['XXM007', 'XXM008']\n",
    "    \n",
    "        label_file = label_dir + sids[1] + \"_\" + str(date) + \"_\" + PMstrs[i] + \"_grid\" + str(GRID_SIZE) + \".csv\"\n",
    "        labels_found = False\n",
    "        \n",
    "        if (os.path.isfile(label_file)):\n",
    "            sid_plot = sids[1]\n",
    "            labels_found = True\n",
    "        else:\n",
    "            label_file = label_dir + sids[0] + \"_\" + str(date) + \"_\" + PMstrs[i] + \"_grid\" + str(GRID_SIZE) + \".csv\"\n",
    "            if (os.path.isfile(label_file)):\n",
    "                sid_plot = sids[0]\n",
    "                labels_found = True\n",
    "        if(labels_found):\n",
    "            pd_df=pd.read_csv(label_file, sep=',',header=None, skiprows=3)\n",
    "            labels = pd_df.values\n",
    "        else:\n",
    "            print(\"no labels found\")\n",
    "\n",
    "        pd_df=pd.read_csv(label_file, sep=',',header=None, skiprows=3)\n",
    "        PM_true = pd_df.values\n",
    "        error = calculateError(PM_true, PM_pred)\n",
    "        errors.append(error)\n",
    "        \n",
    "        if (MAPPING_ON):\n",
    "            map_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/images/maps/interpolations/CNN/\"+str(date)+\"/\"\n",
    "            maxPM = np.max(PM_pred)\n",
    "            PM_index = 1\n",
    "            pred_map = folium.Map(location=[55.943, -3.19], zoom_start=15,tiles=\"Stamen Toner\")\n",
    "            labelAndPlotPersonalData(map_dir, PM_pred, sid_plot, PM_index, date)\n",
    "        \n",
    "        r2 = calculateR2(PM_true, PM_pred)\n",
    "        r2s.append(r2)\n",
    "    \n",
    "    overallError = np.average(errors)    \n",
    "    print(\" kernel size : , \" + str(kernel_size) + \" , \" + str(overallError))\n",
    "    \n",
    "    ML_train_cost = ML_dir + \"PM2.5_train_cost.csv\"\n",
    "    ML_val_cost = ML_dir + \"PM2.5_valid_cost.csv\"\n",
    "    train_df=pd.read_csv(ML_train_cost, sep=' ',header=None)\n",
    "    valid_df=pd.read_csv(ML_val_cost, sep=' ',header=None)\n",
    "    plt.figure() \n",
    "    valid_df.plot()\n",
    "    ax_val = valid_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for interpolation and machine learning MAPE calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for interpolation and machine learning MAPE calculations\n",
    "def interpolate_PM(start_date, PM_index, toDelete):\n",
    "    PM = []\n",
    "    labels_to_plot = []\n",
    "    x, y, z = readStaticData(start_date, PM_index)\n",
    "    \n",
    "    label_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/data/label/\"+str(start_date)+\"/\"\n",
    "    sids = ['XXM007', 'XXM008']\n",
    "    \n",
    "    labels = []\n",
    "\n",
    "    if (DOUBLE_COLLECTION):\n",
    "        index_interp = random.choice((0,1))\n",
    "        index_plot = not index_interp\n",
    "        \n",
    "        sid_interp = sids[index_interp]\n",
    "        sid_plot = sids[index_plot]\n",
    "\n",
    "        plot_filename = label_dir + sid_plot + \"_\" + str(start_date) + \"_\" + PMstrs[PM_index] + \"_grid\" + str(GRID_SIZE) + \".csv\"\n",
    "        if (os.path.isfile(plot_filename)):\n",
    "            labels_to_plot.append(sid_plot)\n",
    "            \n",
    "        interp_filename = label_dir + sid_interp + \"_\" + str(start_date) + \"_\" + PMstrs[PM_index] + \"_grid\" + str(GRID_SIZE) + \".csv\"\n",
    "        if (os.path.isfile(interp_filename)):\n",
    "            x, y, z = readPersonalData(x, y, z, start_date, sid_interp, PM_index)\n",
    "    \n",
    "    interp_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/data/train/\"+str(start_date)+\"/\"+ PMstrs[PM_index] + \"/\"\n",
    "    if not os.path.exists(interp_dir):\n",
    "        os.makedirs(interp_dir)\n",
    "            \n",
    "    xnew = np.arange(0, GRID_SIZE, 1.0)\n",
    "    ynew = np.arange(0, GRID_SIZE, 1.0)\n",
    "    \n",
    "    if (INTERP_TYPE == 0):\n",
    "        f = interpolate.interp2d(x, y, z, kind='linear')\n",
    "        filename = interp_dir  + str(start_date) + \"_grid\" + str(GRID_SIZE) + \"_interp2D.csv\"\n",
    "        map_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/images/maps/interpolations/interp2d/\"+str(start_date)+\"/\"\n",
    "        PM_pred = f(xnew, ynew)\n",
    "\n",
    "    elif (INTERP_TYPE == 1):\n",
    "        f = interpolate.SmoothBivariateSpline(x, y, z, kx=1, ky=1)\n",
    "        filename = interp_dir + str(start_date) + \"_grid\" + str(GRID_SIZE) + \"_smoothspline.csv\"\n",
    "        map_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/images/maps/interpolations/SmoothBivariate/\"+str(start_date)+\"/\"\n",
    "        PM_pred = f(xnew, ynew)\n",
    "    \n",
    "    elif (INTERP_TYPE == 2):\n",
    "        filename = interp_dir + str(start_date) + \"_grid\" + str(GRID_SIZE) + \"_universalKriging.csv\"\n",
    "        map_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/images/maps/interpolations/UniversalKriging/\"+str(start_date)+\"/\"\n",
    "        UK = UniversalKriging(x, y, z, variogram_model='linear',drift_terms=['regional_linear'])\n",
    "        znew, ss = UK.execute('grid', xnew, ynew)\n",
    "        PM_pred = znew\n",
    "    \n",
    "    elif (INTERP_TYPE == 3):\n",
    "        filename = interp_dir + str(start_date) + \"_grid\" + str(GRID_SIZE) + \"_ordinaryKriging.csv\"\n",
    "        map_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/images/maps/interpolations/OrdinaryKriging/\"+str(start_date)+\"/\"\n",
    "        OK = OrdinaryKriging(x, y, z, variogram_model='linear', verbose=False, enable_plotting=False)\n",
    "        znew, ss = OK.execute('grid', xnew, ynew)\n",
    "        PM_pred = znew\n",
    "        \n",
    "    elif (INTERP_TYPE == 4):\n",
    "        rbfi = Rbf(x, y, z, function='gaussian')\n",
    "        PM_pred = rbfi(xnew, ynew)\n",
    "        filename = interp_dir + str(start_date) + \"_grid\" + str(GRID_SIZE) + \"_rbf.csv\"\n",
    "        map_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/images/maps/interpolations/RBF/\"+str(start_date)+\"/\"\n",
    "\n",
    "        PM_pred = np.zeros((xnew.shape[0], ynew.shape[0]))\n",
    "        for i in range(xnew.shape[0]):\n",
    "            for j in range(ynew.shape[0]):\n",
    "                PM_pred[j,i] = rbfi(xnew[i], ynew[j])\n",
    "\n",
    "    if not os.path.exists(map_dir):\n",
    "        os.makedirs(map_dir)\n",
    "\n",
    "    PM_pred[PM_pred < 0.0] = 0.0 #Shouldn't have negative predictions\n",
    "    maxPM = np.max(PM_pred)\n",
    "\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, lineterminator='\\n')\n",
    "        writer.writerows(PM_pred)\n",
    "\n",
    "    if not DOUBLE_COLLECTION:\n",
    "        filename = label_dir + sids[0] + \"_\" + str(start_date) + \"_\" + PMstrs[1] + \"_grid\" + str(GRID_SIZE) + \".csv\"\n",
    "        if (os.path.isfile(filename)):\n",
    "            sid_plot = sids[0]\n",
    "        else:\n",
    "            sid_plot = sids[1]\n",
    "    labelAndPlotPersonalData(map_dir, PM_pred, sid_plot, PM_index, start_date)\n",
    "    \n",
    "def readStaticData(start_date, PM_index):\n",
    "    PM = []\n",
    "\n",
    "    train_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/data/train/\"+str(start_date)+\"/\"\n",
    "\n",
    "    filename = train_dir + str(start_date) + \"_grid\" + str(GRID_SIZE) + \".csv\"\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        medianTime = pd.to_datetime(next(reader)[0])\n",
    "        averageTemp = next(reader)[0]\n",
    "        averageHum = next(reader)[0]\n",
    "    \n",
    "        PM.append(next(reader))\n",
    "        PM.append(next(reader))\n",
    "        PM.append(next(reader))\n",
    "    \n",
    "    x = np.zeros(6)\n",
    "    y = np.zeros(6)\n",
    "    z = np.zeros(6)\n",
    "    \n",
    "    for num, coord in enumerate(staticCoords):\n",
    "        col = math.floor((topLat - coord[0]) / heightInterval)\n",
    "        row = math.floor((coord[1] - leftLon) / widthInterval)\n",
    "        x[num] = row\n",
    "        y[num] = col\n",
    "        z[num] = PM[PM_index][num] \n",
    "    \n",
    "    return x, y, z\n",
    "\n",
    "def readPersonalData(x_old, y_old, z_old, start_date, sid_interp, PM_index):\n",
    "    labels = []\n",
    "    label_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/data/label/\"+str(start_date)+\"/\"\n",
    "    PMstrs = ['PM1', 'PM2.5', 'PM10']\n",
    "\n",
    "    filename = label_dir + sid_interp + \"_\" + str(start_date) + \"_\" + PMstrs[PM_index] + \"_grid\" + str(GRID_SIZE) + \".csv\"\n",
    "    pd_df=pd.read_csv(filename, sep=',',header=None, skiprows=3)\n",
    "    PM_true = pd_df.values\n",
    "    \n",
    "    label_count = (PM_true >= 0).sum() + 1\n",
    "    \n",
    "    x_new = np.zeros(label_count)\n",
    "    y_new = np.zeros(label_count)\n",
    "    z_new = np.zeros(label_count)\n",
    "    \n",
    "    index = 0\n",
    "    for col in range(GRID_SIZE):\n",
    "        for row in range(GRID_SIZE):\n",
    "            PMval = PM_true[row][col]\n",
    "            if (PMval >= 0):\n",
    "                index += 1\n",
    "                x_new[index] = row\n",
    "                y_new[index] = col\n",
    "                z_new[index] = PMval\n",
    "                \n",
    "    x = np.concatenate((x_old, x_new))\n",
    "    y = np.concatenate((y_old, y_new))\n",
    "    z = np.concatenate((z_old, z_new))\n",
    "    return x, y, z\n",
    "    \n",
    "def labelAndPlotPersonalData(map_dir, PM_pred, sid_plot, PM_index, start_date):\n",
    "    label_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/data/label/\"+str(start_date)+\"/\"\n",
    "    labels = []\n",
    "    filename = label_dir + sid_plot + \"_\" + str(start_date) + \"_\" + PMstrs[PM_index] + \"_grid\" + str(GRID_SIZE) + \".csv\"\n",
    "\n",
    "    if (os.path.isfile(filename)):\n",
    "        if (MAPPING_ON):\n",
    "            pred_map = folium.Map(location=[55.943, -3.19], zoom_start=15,tiles=\"Stamen Toner\")\n",
    "            final_map_dir = map_dir + sid_plot\n",
    "            createMap(pred_map, start_date, sid_plot, PM_pred)\n",
    "            pred_map.save(map_dir + str(start_date) + \"_\" + str(sid_plot) + \".html\")\n",
    "            \n",
    "        pd_df=pd.read_csv(filename, sep=',',header=None, skiprows=3)\n",
    "        PM_true = pd_df.values\n",
    "        error = calculateError(PM_true, PM_pred)\n",
    "        errors.append(error)\n",
    "        r2s.append(calculateR2(PM_true, PM_pred))\n",
    "\n",
    "def calculateError(PM_true, PM_pred):\n",
    "    mask = PM_true > 0\n",
    "    PM_mask = PM_true[mask]\n",
    "    PM_diff = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    errors2 = []\n",
    "\n",
    "    for row in range(GRID_SIZE):\n",
    "        for col in range(GRID_SIZE):\n",
    "            if (PM_true[row][col] > 0):\n",
    "                PM_diff[row][col] = PM_true[row][col] - PM_pred[row][col]\n",
    "                errors2.append(np.abs(PM_diff[row][col]) / PM_true[row][col])\n",
    "            else:\n",
    "                PM_diff[row][col] = 0\n",
    "                PM_true[row][col] = 0\n",
    "                \n",
    "    average_error = np.average(errors2) * 100\n",
    "    return average_error\n",
    "\n",
    "def calculateR2(PM_true, PM_pred):\n",
    "    mean_true = np.average(PM_true)\n",
    "    \n",
    "    PM_SStot = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    PM_SSres = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    \n",
    "    for row in range(GRID_SIZE):\n",
    "        for col in range(GRID_SIZE):\n",
    "            if (PM_true[row][col] > 0):\n",
    "                PM_SStot[row][col] = (PM_true[row][col] - mean_true) ** 2\n",
    "                PM_SSres[row][col] = (PM_true[row][col] - PM_pred[row][col]) ** 2\n",
    "                \n",
    "    r2 = 1 - (np.sum(PM_SSres) / np.sum(PM_SStot))\n",
    "    return r2\n",
    "                        \n",
    "def createMap(mapObj, start_date, sid, PM_pred):\n",
    "    end_date = getEndDate(start_date)\n",
    "    data_dir = \"/Users/zoepetard/Google Drive/Edinburgh/MscProj/FillingTheGaps/data/raw/personal/\"+str(start_date)+\"-\"+str(end_date)+\"/\"\n",
    "    sids = ['XXM007', 'XXM008']\n",
    "    pdata = data_downloader.readAirSpeckPCSV(start_date, end_date, data_dir)\n",
    "    belt_index = sids.index(sid)\n",
    "    \n",
    "    maxPM = np.max(PM_pred)\n",
    "    minPM = np.min(PM_pred)\n",
    "    \n",
    "    ##Add prediction cells\n",
    "    for num, cell in enumerate(cells):\n",
    "        row = int(num / GRID_SIZE)\n",
    "        col = int(num % GRID_SIZE)\n",
    "        PM = PM_pred[row][col]\n",
    "        cell_multi = cell[0][\"geometry\"]\n",
    "        osm_reader.addCellPM(mapObj, cell_multi, PM, maxPM, minPM)\n",
    "            \n",
    "    ##Add validation walk\n",
    "    for j in range(len(pdata[belt_index])):\n",
    "        folium.CircleMarker((pdata[belt_index][\"latitude\"][j], pdata[belt_index][\"longitude\"][j]),\n",
    "                    radius=5,\n",
    "                    color='#000000',\n",
    "                    weight=1.0,\n",
    "                    fill_color=osm_reader.assignColor(pdata[belt_index][\"PM2.5\"][j], maxPM, minPM),\n",
    "                    fill=True,\n",
    "                    fill_opacity=1.0,\n",
    "                   ).add_to(mapObj)\n",
    "\n",
    "def getEndDate(start_date):\n",
    "    return start_date + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
